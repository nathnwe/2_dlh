{"cells":[{"cell_type":"markdown","id":"a26908b2-85bc-4304-8fdd-e23014cc00cb","metadata":{"id":"a26908b2-85bc-4304-8fdd-e23014cc00cb"},"source":["# Practical 2: Training ANNs and CNNs\n","\n","Contents:\n","* [1. Loading and Preparing the data](#1.-Loading-and-Preparing-the-data)\n","* [2. PyTorch Basics](#2.-PyTorch-Basics)\n","* [3. Designing a basic ANN](#3.-Designing-a-Basic-ANN)\n","* [4. Designing a basic CNN](#4.-Designing-a-Basic-CNN)\n","* [5. Extra Considerations](#5.-Extra-Considerations)\n","\n","We will begin by considering the very basics needed for implementing a neural network. We will then explore training an ANN and CNN for the same pneumonia data we used in the last practical.\n","\n","Throughout this practical, we will be using the ```PyTorch``` package to create and train our networks. The documentation has lots of information about the functions and their implementation. If you want more guidance on using ```PyTorch``` tutorials can be found [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n","\n","Look out for the following throughout the practical:\n","\n","‚úÖ To do: places where you need to add code throughout. Make sure that you understand what is going on before moving on to the next stage.\n","\n","üè• Indicates particular considerations when working with medical imaging data.\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"3wwYlfGJyNFv"},"id":"3wwYlfGJyNFv","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7b12d3ae-ba86-44c6-8e2f-324c18868e19","metadata":{"id":"7b12d3ae-ba86-44c6-8e2f-324c18868e19"},"outputs":[],"source":["# Import dependencies\n","# Here we load the packages that we need for the rest of the practical\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from mpl_toolkits import mplot3d\n","import warnings\n","warnings.filterwarnings(action='ignore', category=FutureWarning)\n","import torch\n","import time\n","from prepare_data import prepare_data"]},{"cell_type":"markdown","id":"a235daf4-3282-4355-8f5f-72682f30055b","metadata":{"id":"a235daf4-3282-4355-8f5f-72682f30055b"},"source":["## 1. Loading and Preparing the data\n","#### 1.1 Loading the data\n","We will be using the same data as last week so we will just run a script to save some time. Check out last weeks practical to remind yourself of the steps."]},{"cell_type":"code","execution_count":null,"id":"78743392-bc57-438b-b1dd-1fb33e41dbd4","metadata":{"id":"78743392-bc57-438b-b1dd-1fb33e41dbd4"},"outputs":[],"source":["pth = '/content/drive/MyDrive/healthcare_practicals/data/pneumoniamnist.npz'  # Path to data\n","[X_train, y_train], [X_val, y_val], [X_test, y_test] = prepare_data(pth_to_data=pth)\n"]},{"cell_type":"markdown","id":"45f7f0cf-3a8f-4699-92e4-df64792b10b0","metadata":{"id":"45f7f0cf-3a8f-4699-92e4-df64792b10b0"},"source":["## 2. PyTorch Basics\n","Although other packages exist, ```PyTorch``` is one of the most popular packages for designing and training neural networks. ```PyTorch``` is a lot like numpy. A lot of operations used to manipulate numpy arrays have their counterparts in PyTorch and numpy arrays can be converted to and from PyTorch *tensors*. PyTorch arrays are given the more proper mathematical name of tensors (see e.g. tensorflow). In terms of practical usage, PyTorch tensors can be manipulated very similarly to numpy‚Äôs ndarrays, with the addition being that tensors can also be used on a GPU to accelerate computing.\n","\n","We first need to identify if we have a cuda device available. This practical will assume that we do not, but when you are training real models you will want to use the cuda device if it is available."]},{"cell_type":"code","execution_count":null,"id":"c028e784-b0a9-4f0b-8515-5881b3fcca1d","metadata":{"id":"c028e784-b0a9-4f0b-8515-5881b3fcca1d"},"outputs":[],"source":["device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","id":"3d7d791b-24df-415a-af76-89618b6b08ae","metadata":{"id":"3d7d791b-24df-415a-af76-89618b6b08ae"},"source":["Let's create a random array of numbers in both ```PyTorch``` and ```numpy```. They are indexed in the same way and can also be reshaped."]},{"cell_type":"code","execution_count":null,"id":"9f31200d-d711-40ce-acce-0ac91c1f057e","metadata":{"id":"9f31200d-d711-40ce-acce-0ac91c1f057e"},"outputs":[],"source":["# Numpy\n","numpy_random_arr = np.random.rand(2,2,2)\n","print('Numpy = ', numpy_random_arr)\n","\n","# PyTorch\n","torch_random_arr = torch.rand(2,2,2)\n","print('Torch = ',torch_random_arr)\n","\n","# Indexing\n","print(numpy_random_arr[0,0,0], torch_random_arr[0,0,0])\n","\n","# Reshaping\n","numpy_random_arr = np.reshape(numpy_random_arr, [4,2])\n","torch_random_arr = torch_random_arr.reshape(4,2)\n","print(numpy_random_arr.shape)\n","print(torch_random_arr.size())"]},{"cell_type":"markdown","id":"2f1e6628-d833-4423-8ae5-d912b22358ba","metadata":{"id":"2f1e6628-d833-4423-8ae5-d912b22358ba"},"source":["Converting between the two types of arrays is easy"]},{"cell_type":"code","execution_count":null,"id":"642a7954-09a8-4c86-bbaa-cc9d60ffb468","metadata":{"id":"642a7954-09a8-4c86-bbaa-cc9d60ffb468"},"outputs":[],"source":["a = np.array([[1,2],[3,4]]) # make a numpy array\n","\n","a_torch = torch.from_numpy(a) #converting to a torch Tensor from a numpy array\n","print(a_torch)\n","\n","a_np = a_torch.numpy() # converting to a numpy array from a torch Tensor\n","print(a_np)"]},{"cell_type":"markdown","id":"24e5d9d7-4412-43bf-ae44-9ad6a56578eb","metadata":{"id":"24e5d9d7-4412-43bf-ae44-9ad6a56578eb"},"source":["Other basic functions such as torch.diag, torch.cat (concatenate), torch.matmul work similarly to their numpy equivalents. <br>\n","As always, when looking for a function **check the [documentation](https://pytorch.org/docs/stable/nn.html)** and consider running through the [official PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html) on tensor manipulation."]},{"cell_type":"markdown","id":"3cf01020-3d01-47ca-9942-1bfe894d3823","metadata":{"id":"3cf01020-3d01-47ca-9942-1bfe894d3823"},"source":["Note that, as with numpy, the data type (dtype) of an object is important. PyTorch tensor types - just like in any other programming language - depend on whether they are storing integers, floating points or bools, and in how many bits. Often, it is important to make sure tensors are of the right / matching type when performing operations on them.\n","See https://pytorch.org/docs/stable/tensors.html for a list of dtypes and what they are called in PyTorch.\n","\n","Changing torch tensor type is simple:\n"]},{"cell_type":"code","execution_count":null,"id":"4bbe9cc1-f964-445a-86b5-6fc03b7f7abd","metadata":{"id":"4bbe9cc1-f964-445a-86b5-6fc03b7f7abd"},"outputs":[],"source":["print(a_torch)\n","print(a_torch.to(torch.double))"]},{"cell_type":"markdown","id":"9be4f164-aac7-4560-93e8-718b77c5617e","metadata":{"id":"9be4f164-aac7-4560-93e8-718b77c5617e"},"source":["#### 2.2 Basic Tensor Operations\n","There is some streamlining of operations in PyTorch relative to numpy which can simplify code development. For example, compare here the multiplications of two arrrays using broadcasting.\n","\n","**To do:**\n","\n","‚úÖ Generate two random numpy arrays, **a** and __b__ of sizes [12,5] and [3,5,20].\n","\n","\n","‚úÖ Find the matrix product **a** $\\cdot$ __b__ using broadcasting.\n","    **Hint: may need to reshape a first:**  multiplication of 3D arrays involves [multiplication of stacks of 2D matrices]( https://www.geeksforgeeks.org/numpy-3d-matrix-multiplication/). Therefore, the final result should be of shape (3,12,20), since multiplication of a ($12 \\times 5$) and a ($5 \\times 20$) matrix returns shape ($12 \\times 20$).\n","    \n","‚úÖ Convert **a** and __b__ into PyTorch Tensors and perform direct multiplication (without reshaping)."]},{"cell_type":"code","execution_count":null,"id":"20e545c7-e930-4039-bc4b-7fa16916e26a","metadata":{"id":"20e545c7-e930-4039-bc4b-7fa16916e26a"},"outputs":[],"source":["a =\n","b =\n","c =\n","print(c)\n","\n","a_t = torch.from_numpy(a)\n","b_t = torch.from_numpy(b)\n","c_t =\n","print(c)"]},{"cell_type":"markdown","id":"54c47854-9fbe-41cc-9722-d4cce4fa21b3","metadata":{"id":"54c47854-9fbe-41cc-9722-d4cce4fa21b3"},"source":["#### 2.3 Autograd: automatic differentiation\n","\n","As you might imagine, it is not the similarities between the two that we are interested in, but what makes torch tensors relevant to machine learning. The most significant and relevant difference is that PyTorch tensors also have an associated *gradient*. It is this that is used to perform the optimization that machine learning is based on.\n","\n","The gradient of a PyTorch tensor is stored as its ```.grad``` attribute. All PyTorch tensors have this even if it is not apparent nor used. In such a case it would be set to \"None\".\n","\n","All PyTorch tensors have another boolean attribute ```requires_grad``` that indicates whether PyTorch *needs* to track and store its gradient or whether it is simply a static tensor. By default, requires_grad is set to False.\n","When we later construct neural networks from the torch.nn neural network module, requires_grad will be automatically set to True for the relevant learning parameters so it is not something you should generally worry about setting manually.\n","\n","In addition the attribute ```grad_fn```: This is the backward function used to calculate the gradient.\n","\n","For example, let's observe gradient estimation for a simple function $L = \\frac{1}{N} \\sum_{i}\\sum_j (2x_{ij}+3)^2  $ operating on a matrix $\\mathbf{X}$ with $N$ elements.\n","\n","**To do:**\n","\n","‚úÖ Run this code and check the outputs. Consider also printing out the `is_leaf` and `requires_grad` attributes for each tensor.\n","\n","‚úÖ Confirm the answer using the chain rule."]},{"cell_type":"code","execution_count":null,"id":"42926ae3-037c-4f7c-88d3-3c2ebd3fdcc8","metadata":{"id":"42926ae3-037c-4f7c-88d3-3c2ebd3fdcc8"},"outputs":[],"source":["X = torch.ones(5,5, requires_grad=True) # generate a random Tensor\n","\n","print('Initial gradient:', X.grad) # check its gradient - the result is None\n","\n","y=2*X+3\n","z=y*y\n","out = z.mean()\n","\n","out.backward()\n","print('grad:', X.grad)\n","print('grad_fn:', out.grad_fn)"]},{"cell_type":"markdown","id":"97a5df48-ec47-4570-b4ce-1d3f21def24d","metadata":{"id":"97a5df48-ec47-4570-b4ce-1d3f21def24d"},"source":["In practice the ```autograd``` package provides an engine to perform backpropagation. As variables and operations are defined, it sets up a dynamic computational graph in the same sense as we saw in our first lecture. In this, the leaves of the graph are input tensors, identified using the attribute ```is_leaf==True```. Roots are output tensors. Gradients are then calculated by tracing the graph from the root to the leaf and multiplying every gradient in the way using the chain rule.\n"]},{"cell_type":"markdown","id":"61d7a8ea-7033-43d6-a7fe-c6e3d1c2557a","metadata":{"id":"61d7a8ea-7033-43d6-a7fe-c6e3d1c2557a"},"source":["#### 2.4. nn.Functional\n","\n","As you will see as we go forward, most PyTorch layers can be implemented either as a `torch.nn.Module` object or as a `torch.nn.functional` function. So which should you use?\n","\n","Essentially,  `nn.functional` provides building block functions (e.g. layers / activations) in the form of functions. This means that they can be directly called on the input rather than defining the object.\n","\n","In cases where we have weights or other trainable parameters e.g. linear or convolutional layers, states which behave differently at train time and test time (for example dropout and Batch Norm), then we should use `nn.Module` objects.\n","\n","On the other hand, in cases where no state or weights are required, `nn.functional` counterparts may be used. Examples being, resizing (nn.functional.interpolate),  average pooling (nn.functional.AvgPool2d) and activation functions.\n"]},{"cell_type":"markdown","id":"f9051db7-5b4d-4f07-9d2b-04c23e93ce57","metadata":{"id":"f9051db7-5b4d-4f07-9d2b-04c23e93ce57"},"source":["# 3. Designing a Basic ANN\n","\n","We are now going to train a basic ANN to classify the pneumonia data. This will require building up the different aspects of the training procedure."]},{"cell_type":"code","execution_count":null,"id":"1910e5c7-3dcf-4c86-92af-2f3545f1ba1b","metadata":{"id":"1910e5c7-3dcf-4c86-92af-2f3545f1ba1b"},"outputs":[],"source":["import torch.nn as nn # Contains all the functions we need to to train our network\n","import torch.nn.functional as F # Contains some additional functions such as activations\n","from torch.autograd import Variable"]},{"cell_type":"markdown","id":"4fe24734-7029-4223-9847-5edde4bce5ac","metadata":{"id":"4fe24734-7029-4223-9847-5edde4bce5ac"},"source":["#### 3.1 Create a data loader\n","\n","In the first task, as for the computer vision example we will create a dataloader to load the images and labels. In Deep Learning, the data must be collected and prepared into batches before being fed into the neural network for training. In many cases, our medical imaging data is too large to all be loaded into memory at once. We may also want to transform (augment) our data either as a pre-processing step or to simulate the creation of bigger data sets. Typically, we want to randomly shuffle our data during training such that our network does not always see data in the same order. PyTorch streamlines this process through the provision of two classes: *DataSet*, and accompanying iterator *DataLoader*.\n","\n","Let's look at the basic structure of the `Dataset` class (https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py) and discuss some of it's optional features.\n","\n","```class Dataset(object):  \n","\n","    \"\"\"An abstract class representing a :class:Dataset.\n","    All datasets that represent a map from keys to data samples should subclass\n","    it. All subclasses should overwrite :meth:__getitem__, supporting fetching a\n","    data sample for a given key. Subclasses could also optionally overwrite\n","    :meth:__len__/, which is expected to return the size of the dataset by many\n","    :class:~torch.utils.data.Sampler implementations and the default options\n","    of :class:~torch.utils.data.DataLoader.\n","    .. note::\n","      :class:~torch.utils.data.DataLoader by default constructs a index\n","      sampler that yields integral indices.  To make it work with a map-style\n","      dataset with non-integral indices/keys, a custom sampler must be provided.\n","    \"\"\"\n","    def __getitem__(self, index):\n","        raise NotImplementedError\n","    def __add__(self, other):\n","        return ConcatDataset([self, other])```\n","\n","\n"]},{"cell_type":"markdown","id":"e5db7817-d6de-444d-838d-bbcc265ceeb0","metadata":{"id":"e5db7817-d6de-444d-838d-bbcc265ceeb0"},"source":["What this states is that any class that inherits from the baseclass must override the following methods:\n","- `__len__` so that len(dataset) returns the size of the dataset.\n","- `__getitem__` which returns a sample from the dataset given an index. For supervised learning from images this requires it to return both an example image and its label.\n","\n","In addition to this it is common to pass a transform argument to the `DataSet` class which will support augmentation of the data. After that you have great freedom as to the actual structure and ordering of the code in the class.\n","\n","The `DataLoader` is an iterator class, which uses the `__getitem__` and `__len__` functions to collate data into batches and sample at random (`shuffle`) from the data referenced by the `Dataset` class. It also supports loading and processing the data in parallel (with the number of parallel processes determined by parameter `num_workers`. **Generally, shuffling the order of the data is very important** as, in this way, the batches between epochs will not look alike, improving generalisation.\n","\n","The generic form of a call to `DataLoader` is\n","\n","`dataloader = DataLoader(transformed_dataset, batch_size=4,shuffle=True, num_workers=4)`\n","                        \n","Unlike the `Dataset` class, this is unlikely to need overloading.\n","\n","We need to create a dataset to prepare our data for training. The data loader is going to take a pair of numpy arrays (X and y) and return the next batch of images and labels. For now we are going to keep our model as simple as possible and so we aren't going to add any augmentation transformations.\n","\n","**To do:**\n","\n","‚úÖ Complete the dataset. This is as simple as a dataset gets, and they can be much more complicated. Hint: the numpy arrays need converting to PyTorch tensors.\n","\n","‚úÖ Think about how we could have incorporated the earlier data preprocessing into the dataloader.\n"]},{"cell_type":"code","execution_count":null,"id":"f221141c-3f3b-41e2-bec7-f8b71a415230","metadata":{"id":"f221141c-3f3b-41e2-bec7-f8b71a415230"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class numpy_dataset(Dataset):  # Inherit from Dataset class\n","    def __init__(self, data, target):\n","        ## Add code here\n","        self.data =\n","        self.target =\n","\n","    def __getitem__(self, index):\n","        x =\n","        y =\n","        return x, y\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","train_dataset = numpy_dataset(X_train, y_train)\n","val_dataset = numpy_dataset(X_val, y_val)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, drop_last=True)"]},{"cell_type":"markdown","id":"0e8dc1b7-97a5-48c3-b7b4-b53c282dedbf","metadata":{"id":"0e8dc1b7-97a5-48c3-b7b4-b53c282dedbf"},"source":["We can now iterate through the dataloader, to obtain each batch, using the ```iter```function.\n","\n","**To do:**\n","\n","‚úÖ Visualise an example and check our data looks as expected. Remember, we have changed the intensity distribution with the ```centring``` function, which means that it won't look identical to at the beginning of the practical but it should still be recognisable! (This is very important if the dataset is more complicated.)"]},{"cell_type":"code","execution_count":null,"id":"f1ea88e6-1a69-4ac3-9c20-877fa28c7f7e","metadata":{"id":"f1ea88e6-1a69-4ac3-9c20-877fa28c7f7e"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"a12d309f-cc67-4f43-8e74-eb8027afe5be","metadata":{"tags":[],"id":"a12d309f-cc67-4f43-8e74-eb8027afe5be"},"source":["#### 3.2 Designing the network\n","\n","It's now time to design our first network. We are first going to design a simple fully connected model using linear layers only.\n","\n","A PyTorch network class is then defined from a minimum of two functions:\n","\n","1. The first part is creation of the constructor `__init__` in which you must define all parameters and layers that you will use. **Note**, you must always call the super() function to initialize and start the parent class.\n","2. The second part is to define the forward pass through the function `forward(self,x)` which puts these layers together to calculate the output.\n","\n","We need to design a simple fully connected architecture following the schematic below:\n","\n","![ANN](https://drive.google.com/uc?id=1jVQm2eGcpttTmjgpkfbRatOBE91gYvDO)\n","\n","**To do:**\n","\n","‚úÖ Complete the model architecture.\n","\n","‚úÖ Identify how the image needs reshaping in the forward function to be passed through the model.\n","\n","‚úÖ Select the best activation function for the last layer. Hint: think about what the output of the last layer represents.\n","\n","‚úÖ Calculate the number of trainable parameters in the model.\n","\n","‚úÖ If we had a cuda device available we would load the model to the cuda device. Work out how we would do this.\n","\n","üè• We normally have much less data than is available in classic computer vision applications. What impact is this likely to have on our architecture choice?\n"]},{"cell_type":"code","execution_count":null,"id":"04d1490b-48c0-43c3-b3fd-318bd9de93b9","metadata":{"id":"04d1490b-48c0-43c3-b3fd-318bd9de93b9"},"outputs":[],"source":["class ANN(nn.Module):\n","   def __init__(self):\n","        super(ANN, self).__init__()\n","        # Add architecture here\n","        self.layer1 =\n","        self.relu1 =\n","        self.layer2 =\n","        self.relu2 =\n","        self.layer3 =\n","        self.relu3 =\n","        self.layer4 =\n","        self.relu4 =\n","        self.layer5 =\n","        self.activation =\n","\n","   def forward(self, x):\n","        # Define forward pass here\n","        # TODO: Reshape the image\n","        x =\n","        x = self.relu1(self.layer1(x))\n","        x = self.relu2(self.layer2(x))\n","        x = self.relu3(self.layer3(x))\n","        x = self.relu4(self.layer4(x))\n","        x = self.layer5(x)\n","        x = self.activation(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"470fac5b-9958-4c20-bd75-3e0e72dd8531","metadata":{"id":"470fac5b-9958-4c20-bd75-3e0e72dd8531"},"outputs":[],"source":["net = ANN()\n","print(net)\n","\n","print('Trainable params: ', params)"]},{"cell_type":"markdown","id":"7c9e7756-266b-4119-8c24-440a1b0d395e","metadata":{"id":"7c9e7756-266b-4119-8c24-440a1b0d395e"},"source":["#### 3.3 Loss Function"]},{"cell_type":"markdown","id":"0caba974-3577-4235-a87f-02c44101eb82","metadata":{"id":"0caba974-3577-4235-a87f-02c44101eb82"},"source":["Now we have defined and initialised our network, we must perform training to optimise the parameters (or weights of the network). For this we must choose a loss function and optimiser:\n","\n","**Loss Function**\n","\n","The loss function is used to measure how well the prediction model is able to predict the expected results. PyTorch already has many standard loss functions in the torch.nn module. For example, you can use the Cross-Entropy Loss to solve a multi-class classification problem, or a mean squared error (MSE) loss for regression.\n","\n","**Optimiser**\n","\n","There are many optmisers available in PyTorch - see https://pytorch.org/docs/stable/optim.html for full list and further examples. We explored some possible choices for these last week.\n","\n","**To do:**\n","\n","‚úÖ Identify the best choice of loss function for a two class classification task and define it ready for training. If we were using a cuda device this would need loading to the cuda device.\n","\n","‚úÖ Define a stochastic gradient descent optimiser with momentum set to 0.9 and a learning rate of 0.001."]},{"cell_type":"code","execution_count":null,"id":"244b09c7-8a5b-45e8-8bd4-080552c2a47c","metadata":{"id":"244b09c7-8a5b-45e8-8bd4-080552c2a47c"},"outputs":[],"source":["class_loss =\n","optim ="]},{"cell_type":"markdown","id":"3aa1c732-bc6f-47c5-8034-631682785cff","metadata":{"id":"3aa1c732-bc6f-47c5-8034-631682785cff"},"source":["#### 3.4 Training Procedure\n","\n","**Forward Pass:**\n","\n","When performing a forward pass you never explicitly call the `forward` function,  rather the inherited torch.nn.Module handles this behind the scenes. For more details see: https://discuss.pytorch.org/t/predict-output-by-model-does-not-need-call-forward/1489\n","\n","Thus the entire forward pass reduces to two lines:\n","\n","```python\n","# Make prediction with forward pass\n","prediction = net(inputs)\n","\n","# Compute loss by calling the loss function\n"," loss = loss_func(prediction, outputs)\n","\n","```\n","\n","**Backpropagation and clearing gradients:**\n","\n","To perform the backpropagation, you call the loss.backward(), followed by optim.step(). **You should clear gradients before every training iteration to ensure there are no gradients remaining**. This is to avoid mixing up gradients between minibatches.\n","\n","```python\n","net.zero_grad() # to clear the existing gradient - this should be done prior to a training pass\n","optim.zero_grad() # to clear gradient in optimizer - this is equivalent to net.zero_grad().\n","loss.backward() # to perform backpropragation - accumulates the gradient (by addition) for each parameter\n","optim.step() # optimizer.step performs a parameter update based on the current gradient (stored in .grad attribute of a parameter) and the update rule.\n","```\n","\n","We want to combine these into a training function. We also need to create a validation function where no training occurs. This allows us to evaluate the performance of the network through training.\n","\n","**To do**\n","\n","‚úÖ Complete the training function. Note: the network is in train mode.\n","\n","‚úÖ Time how long an epoch of training takes.\n","\n","‚úÖ Complete the validation function. Note: the network is in eval mode. Eval mode affects the peformance of some functions but none of the ones included in this model.\n"]},{"cell_type":"code","execution_count":null,"id":"ef164eb9-391e-4535-98a7-f370e768f0e8","metadata":{"id":"ef164eb9-391e-4535-98a7-f370e768f0e8"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","def train(net, dataloader, optim, loss_func, epoch):\n","    net.train()  #Put the network in train mode\n","    total_loss = 0\n","    pred_store = []\n","    true_store = []\n","\n","    batches = 0\n","\n","    t0 = time.time()\n","    for batch_idx, (data, target) in enumerate(dataloader):\n","\n","        data, target = Variable(data), Variable(target)\n","        batches += 1\n","\n","        # Define training process here:\n","\n","\n","\n","        total_loss += loss\n","        pred_store.append(np.argmax(pred.detach().numpy(), axis=1))\n","        true_store.append(np.argmax(target.detach().numpy(), axis=1))\n","\n","        if batch_idx % 100 == 0: #Report stats every x batches\n","                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                    epoch, (batch_idx+1) * len(data), len(dataloader.dataset),\n","                           100. * (batch_idx+1) / len(dataloader), loss.item()), flush=True)\n","    t1 = time.time()\n","\n","    av_loss = total_loss / batches\n","    av_loss = av_loss.detach().cpu().numpy()\n","\n","    pred_store = np.array(pred_store).reshape(-1)\n","    true_store = np.array(true_store).reshape(-1)\n","    acc = accuracy_score(pred_store, true_store)\n","\n","    print('\\nTraining set: Average loss: {:.4f}'.format(av_loss,  flush=True))\n","    print('Training set: Average Acc: {:.4f}'.format(acc,  flush=True))\n","\n","    total_time = t1-t0\n","    print('Time for epoch = ', total_time)\n","\n","    return av_loss, acc\n","\n","def val(net, val_dataloader, optim, loss_func, epoch):\n","    net.eval()  #Put the model in eval mode\n","    total_loss = 0\n","    pred_store = []\n","    true_store = []\n","\n","    batches = 0\n","\n","    with torch.no_grad():  # So no gradients accumulate\n","        for batch_idx, (data, target) in enumerate(val_dataloader):\n","            batches += 1\n","            data, target = Variable(data), Variable(target)\n","            # Complete validation loop here:\n","\n","\n","            total_loss += loss\n","            pred_store.append(np.argmax(pred.detach().numpy(), axis=1))\n","            true_store.append(np.argmax(target.detach().numpy(), axis=1))\n","        av_loss = total_loss / batches\n","\n","    av_loss = av_loss.detach().numpy()\n","\n","    pred_store = np.array(pred_store).reshape(-1)\n","    true_store = np.array(true_store).reshape(-1)\n","    acc = accuracy_score(pred_store, true_store)\n","\n","    print('Validation set: Average loss: {:.4f}'.format(av_loss,  flush=True))\n","    print('Validation set: Average Acc: {:.4f}'.format(acc,  flush=True))\n","    print('\\n')\n","\n","    return av_loss, acc"]},{"cell_type":"markdown","id":"ff61f89c-8e4a-458c-b8f8-85b8fb91adfc","metadata":{"id":"ff61f89c-8e4a-458c-b8f8-85b8fb91adfc"},"source":["Now all we need to do is run these for several epochs and see how the model does!"]},{"cell_type":"code","execution_count":null,"id":"88266163-bd9b-40c5-8102-254c18b4aaae","metadata":{"id":"88266163-bd9b-40c5-8102-254c18b4aaae"},"outputs":[],"source":["losses = []\n","max_epochs = 10\n","for epoch in range(1, max_epochs+1):\n","    train_loss, train_acc = train(net, train_dataloader, optim, class_loss, epoch)\n","    val_loss, val_acc = val(net, val_dataloader, optim, class_loss, epoch)\n","    losses.append([train_loss, train_acc, val_loss, val_acc])"]},{"cell_type":"code","execution_count":null,"id":"99a4ff8e-c116-4623-9685-a520e2b89ea3","metadata":{"id":"99a4ff8e-c116-4623-9685-a520e2b89ea3"},"outputs":[],"source":["losses = np.array(losses).T\n","print(losses.shape)\n","epochs = np.linspace(1, max_epochs, max_epochs)\n","\n","plt.figure(figsize=(10, 6))\n","plt.subplot(1,2,1)\n","plt.plot(epochs, losses[0,:])\n","plt.plot(epochs, losses[2,:])\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.subplot(1,2,2)\n","plt.plot(epochs, losses[1,:])\n","plt.plot(epochs, losses[3,:])\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(['Train', 'Validation'])\n"]},{"cell_type":"markdown","id":"c64ac4ea-665f-46d5-b05d-a75e79e94aa8","metadata":{"tags":[],"id":"c64ac4ea-665f-46d5-b05d-a75e79e94aa8"},"source":["#### 3.5 Evaluating performance on the held out data\n","That looks okay! We finally need to evaluate the performance of the model on the unseen test data. This allows us to compare between models with data that has never been seen by the model.\n","\n","**To do:**\n","\n","‚úÖ Create a test dataset and dataloader.\n","\n","‚úÖ Complete the predict function.\n","\n","‚úÖ Evaluate the performance of the network on the test data.\n","\n","üè• Last week we looked at the FN rate to consider the network performance. Create a function to return the confusion matrix, so that we can evaluate the frequency of the types of errors being returned by the model, rather than just the accuracy.  (Don't use the inbuilt function!)"]},{"cell_type":"code","execution_count":null,"id":"b94e3e53-0c54-4666-aa82-a247c5ba289a","metadata":{"id":"b94e3e53-0c54-4666-aa82-a247c5ba289a"},"outputs":[],"source":["test_dataset =\n","test_dataloader ="]},{"cell_type":"code","execution_count":null,"id":"29ead732-5cf3-4bfd-b1f1-84b26f1a9acd","metadata":{"id":"29ead732-5cf3-4bfd-b1f1-84b26f1a9acd"},"outputs":[],"source":["def predict(net, test_dataloader):\n","    net.eval()  #Put the model in eval mode\n","    pred_store = []\n","    true_store = []\n","\n","\n","\n","    return pred_store, true_store\n","\n","def confusion_matrix(pred, true):\n","    assert pred.shape == true.shape\n","\n","    return cm"]},{"cell_type":"code","execution_count":null,"id":"ac1ffde1-597e-478b-b3fc-53a9dfa369d7","metadata":{"id":"ac1ffde1-597e-478b-b3fc-53a9dfa369d7"},"outputs":[],"source":["pred, true = predict(net, test_dataloader)\n","acc = accuracy_score(pred, true)\n","print('Test Accuracy = ', acc)\n","cm = confusion_matrix(pred, true)\n","print(cm)"]},{"cell_type":"markdown","id":"cac3110b-8f97-4dd7-88a7-a65744c7a104","metadata":{"id":"cac3110b-8f97-4dd7-88a7-a65744c7a104"},"source":["#### 3.6 Selecting hyperparameters\n","The test accuracy is quite a lot lower than the maximum performance that was reached on the training set... This suggests that the model is overfitting to the training data. This model has lots of hyperparameters we can change to try to reduce this effect.\n","\n","**To do:**\n","\n","‚úÖ Using the cell below, try to maximise the performance on the validation data.\n","\n","‚úÖ Does increasing the number of epochs improve the peformance? How would you identify overfitting and select a better model?\n","\n","‚úÖ Try changing the network design (but only as an ANN) and see what effect this has on the model performance.\n","\n","‚úÖ Once you select the **best** model on the validation data, predict on the test model. This is the final performance of the ANN model. It is important that the test data is held out until model selection is completed.\n"]},{"cell_type":"code","execution_count":null,"id":"7830e568-dbfc-4809-acee-cb42d467a6ee","metadata":{"id":"7830e568-dbfc-4809-acee-cb42d467a6ee"},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, drop_last=True)\n","\n","net = ANN()\n","\n","class_loss = nn.BCELoss()\n","optim = torch.optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)\n","\n","losses = []\n","max_epochs = 20\n","for epoch in range(1, max_epochs+1):\n","    train_loss, train_acc = train(net, train_dataloader, optim, class_loss, epoch)\n","    val_loss, val_acc = val(net, val_dataloader, optim, class_loss, epoch)\n","    losses.append([train_loss, train_acc, val_loss, val_acc])\n","\n","losses = np.array(losses).T\n","print(losses.shape)\n","its = np.linspace(1, max_epochs, max_epochs)\n","\n","plt.figure(figsize=(10, 6))\n","plt.subplot(1,2,1)\n","plt.plot(its, losses[0,:])\n","plt.plot(its, losses[2,:])\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.subplot(1,2,2)\n","plt.plot(its, losses[1,:])\n","plt.plot(its, losses[3,:])\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(['Train', 'Validation'])"]},{"cell_type":"code","execution_count":null,"id":"2f4c1b79-aafa-415a-bc16-d7fa23f7929b","metadata":{"id":"2f4c1b79-aafa-415a-bc16-d7fa23f7929b"},"outputs":[],"source":["# Only run this cell once when you have your final best performing model!\n","\n","pred, true = predict(net, test_dataloader)\n","acc = accuracy_score(pred, true)\n","print('Test Accuracy = ', acc)\n","cm = confusion_matrix(pred, true)\n","print(cm)"]},{"cell_type":"markdown","id":"00fd9b10-59d1-4b84-b5a2-09ff6f58767f","metadata":{"id":"00fd9b10-59d1-4b84-b5a2-09ff6f58767f"},"source":["**To do:**\n","\n","‚úÖ Add your final performance to the board so we can see what the best performance reached was."]},{"cell_type":"markdown","id":"00b23f9d-0ea8-41e3-898b-ba217b15d01e","metadata":{"id":"00b23f9d-0ea8-41e3-898b-ba217b15d01e"},"source":["## 4. Designing a Basic CNN\n","ANNs contain many many parameters due to the fully connected layers, which led to the introduction of CNNs with convolutional layers.\n","#### 4.1 Designing a CNN\n","The network we just trained was a simple fully connected neural network. We are now going to train a CNN in the same way. Consider the architecture below:\n","\n","![CNN](https://drive.google.com/uc?id=1J7LQZgoKUktTTWcEhHOCKc5uIHZQe7sM)\n","\n","where f is the number of features and is a hyperparameter we can change. Note that as the resolution halves the number of feature representations doubles: this is a commonly used base framework.\n","\n","CNN architectures are more complicated than the ANN we defined above so it often is best to use the ```Sequential``` method to define some parts of the network.\n","\n","**To do**:\n","\n","‚úÖ Define the ```_block``` function for the convolutional layers. All convolutional layers should have kernels of size (3,3) and padding should be used to maintain image dimensions. Why is a kernel size of 3x3 the most common choice?\n","\n","‚úÖ Work out how to reshape the image to pass it into the fully connected layers."]},{"cell_type":"code","execution_count":null,"id":"b2f2983c-6ff4-482a-be0c-a15c7a16841a","metadata":{"id":"b2f2983c-6ff4-482a-be0c-a15c7a16841a"},"outputs":[],"source":["from collections import OrderedDict\n","\n","class CNN(nn.Module):\n","    def __init__(self, init_features=4):\n","        super(CNN, self).__init__()\n","        # Convolutional layers\n","        features = init_features\n","        self.encoder1 = CNN._block(1, features, name=\"conv1\")\n","        self.pool1 = nn.MaxPool2d(2, stride=3)\n","        self.encoder2 = CNN._block(features, 2*features, name=\"conv2\")\n","        self.pool2 = nn.MaxPool2d(2, stride=3)\n","        self.encoder3 = CNN._block(2*features, 4*features, name=\"conv3\")\n","\n","        # Fully connected layers\n","        self.fc = nn.Sequential()\n","        self.fc.add_module('f_fc1', nn.Linear(, 120))\n","        self.fc.add_module('f_relu1', nn.ReLU(True))\n","        self.fc.add_module('f_fc2', nn.Linear(120, 16))\n","        self.fc.add_module('f_relu2', nn.ReLU(True))\n","        self.fc.add_module('f_fc3', nn.Linear(16, 2))\n","        self.fc.add_module('f_pred', nn.Softmax(dim=1))\n","\n","    def forward(self, x):\n","        dec1 = self.encoder1(x)\n","        dec2 = self.encoder2(self.pool1(dec1))\n","        dec3 = self.encoder3(self.pool2(dec2))\n","        dec3 = dec3.view(-1, dec3.size()[1]*dec3.size()[2]*dec3.size()[3])\n","        pred = self.fc(dec3)\n","        return pred\n","\n","    @staticmethod\n","    def _block(in_channels, features, name):\n","        return nn.Sequential(\n","            OrderedDict(\n","                [(\n","                    )\n","                ]))"]},{"cell_type":"markdown","id":"d37ce58b-00e4-4319-ab50-c36d22d5bf26","metadata":{"id":"d37ce58b-00e4-4319-ab50-c36d22d5bf26"},"source":["**To do:**\n","\n","‚úÖ Calculate the number of parameters in the model. How does this compare to the ANN?\n"]},{"cell_type":"code","execution_count":null,"id":"c56f6771-8e4a-44ad-9d61-e11faff59749","metadata":{"id":"c56f6771-8e4a-44ad-9d61-e11faff59749"},"outputs":[],"source":["net = CNN(init_features = 4)\n","\n","print('Trainable params: ', params)"]},{"cell_type":"markdown","id":"ebe071de-1ffb-4b04-8b59-1b2cd3803f52","metadata":{"id":"ebe071de-1ffb-4b04-8b59-1b2cd3803f52"},"source":["#### 4.2 Optimising the CNN\n","\n","The model can now be trained using the same training functions we used to train the ANN (this is true as both networks are simple feedforward architectures which require the same inputs and return the same outputs).\n","\n","**To do:**\n","\n","‚úÖ Try to maximise the performance of the CNN.\n","\n","‚úÖ Plot the performance as the number of features is changed. How does the peformance vary with the number of parameters? How does this compare to the ANN?\n","\n","üè• Remove the batch norm from the model and see how the performance is affected. Can you explain why batch norm is so useful in the model?\n","\n","‚úÖ Test the final model on the held out test set and add the performance to the board."]},{"cell_type":"code","execution_count":null,"id":"e8dd1bd4-74cf-4165-b507-e9e328bc2841","metadata":{"id":"e8dd1bd4-74cf-4165-b507-e9e328bc2841"},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, drop_last=True)\n","\n","net = CNN(init_features = 32)\n","\n","class_loss = nn.BCELoss()\n","optim = torch.optim.SGD(net.parameters(), lr = 0.001)\n","\n","losses = []\n","max_epochs = 50\n","for epoch in range(1, max_epochs+1):\n","    train_loss, train_acc = train(net, train_dataloader, optim, class_loss, epoch)\n","    val_loss, val_acc = val(net, val_dataloader, optim, class_loss, epoch)\n","    losses.append([train_loss, train_acc, val_loss, val_acc])\n","\n","losses = np.array(losses).T\n","print(losses.shape)\n","its = np.linspace(1, max_epochs, max_epochs)\n","\n","plt.figure(figsize=(10, 6))\n","plt.subplot(1,2,1)\n","plt.plot(its, losses[0,:])\n","plt.plot(its, losses[2,:])\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.subplot(1,2,2)\n","plt.plot(its, losses[1,:])\n","plt.plot(its, losses[3,:])\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(['Train', 'Validation'])"]},{"cell_type":"code","execution_count":null,"id":"035817ca-095d-4985-9edf-a5e3179fa205","metadata":{"id":"035817ca-095d-4985-9edf-a5e3179fa205"},"outputs":[],"source":["pred, true = predict(net, test_dataloader)\n","acc = accuracy_score(pred, true)\n","print('Test Accuracy = ', acc)\n","cm = confusion_matrix(pred, true)\n","print(cm)"]},{"cell_type":"markdown","id":"41c23865","metadata":{"id":"41c23865"},"source":["# 5. Extra Considerations\n","\n","üè• Explain in your own words what the difference is between a ANN and a CNN. Why do we prefer CNNs over ANNs for image data as input?\n","\n","üè• What is the role of the activation functions? Why is ReLU a common choice?\n","\n","üè• What is the role of the pooing functions? Why is max pooling the most common choice in CNNs?\n","\n","üè• Why might we use a CNN layer with a filter of size 1x1 (for 2d images)?\n","\n","üè• What is meant by the receptive field of a CNN? When might we want to increase the receptive field of the network and how would we achieve this?\n","\n","üè• Medical images are often 3D and large, meaning that we can only set very small batchsizes. What impact do you think having a very small batchsize might have on a) training speed b) generalisation of the model?\n","\n","üè• How would a very small batchsize affect the use of batch norm in the model?\n","\n","üè• Can you think of an application where minimising the number of false positives would be critical?"]},{"cell_type":"code","execution_count":null,"id":"a7db0afe","metadata":{"id":"a7db0afe"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"toc-showcode":false,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}