{"cells":[{"cell_type":"markdown","id":"17432091-3e18-482b-945b-ad1195d3b7a5","metadata":{"id":"17432091-3e18-482b-945b-ad1195d3b7a5","tags":[]},"source":["# Practical 1: Optimisation\n","\n","Contents:\n","* [1. Loading and Preparing the data](#1.-Loading-and-Preparing-the-data)\n","* [2. Logistic Regression Example](#2.-A-Single-Neuron-Logistic-Regression-Classifier)\n","* [3. Extra Considerations](#3.-Extra-Considerations)\n","\n","\n","Throughout this practical, we will be using the ```numpy``` package for calculations, which is designed for matrix multiplication. We will therefore be working with all of the data in vectorised form.\n","\n","Look out for the following throughout the practical:\n","\n","✅ To do: places where you need to add code throughout. Make sure that you understand what is going on before moving on to the next stage.\n","\n","🏥 Indicates particular considerations when working with medical imaging data.\n"]},{"cell_type":"code","execution_count":2,"id":"NsdggTR1OJfs","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18832,"status":"ok","timestamp":1705659896000,"user":{"displayName":"Nicola & Tom","userId":"02954767357562964474"},"user_tz":0},"id":"NsdggTR1OJfs","outputId":"62b95f6c-8574-447b-93af-a81e642ee6aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"id":"9ccbec82-9995-4ed4-a2eb-907cde9def19","metadata":{"id":"9ccbec82-9995-4ed4-a2eb-907cde9def19"},"outputs":[],"source":["# Import dependencies\n","# Here we load the packages that we need for the rest of the practical\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from mpl_toolkits import mplot3d\n","import warnings\n","warnings.filterwarnings(action='ignore', category=FutureWarning)"]},{"cell_type":"markdown","id":"3530ed72-add3-416f-84c6-effb8cca38b9","metadata":{"id":"3530ed72-add3-416f-84c6-effb8cca38b9"},"source":["## 1. Loading and Preparing the data\n","One of the most vital considerations when using deep learning for medical imaging is understanding the dataset we are working with. Without understanding the data, we will struggle to create good models and will not be able to evaluate their performance appropriately."]},{"cell_type":"markdown","id":"e17ef61c-ba0f-45ee-ba76-b1040890de28","metadata":{"id":"e17ef61c-ba0f-45ee-ba76-b1040890de28"},"source":["## 1.1 Loading the data\n","The first thing to do is to load the data. We are going to consider chest X-Ray images with two classes, pneumonia or healthy. Details about the data can be found [here](https://medmnist.com/).\n","\n","Preparing our data is a vital stage in creating any network. We must:\n","* Eyeball the data to make sure it looks as expected\n","* Split the data into training / validation / test splits appropriately so that there is no leakage\n","* Preprocess the data so that it is in the appropriate form for the model\n","\n","In this case, the data has been curated into a ```.npz``` file, and has already been split into train / val / test splits for us.\n","Label 0 = healthy, Label 1 = Pneumonia\n","\n","**To do:**\n","\n","✅ Identify how much data is available in each split\n","\n","✅ Identify how much of each class is available. Very imbalanced numbers in classes is common but can make training networks a challenge.\n","\n","✅ Visualise example of the images, using matplotlib, for both labels"]},{"cell_type":"code","execution_count":6,"id":"29857b7c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/nathanewer/nwe/2_dlh/dlh_practical_jan26/healthcare_practicals/p1-Optimisation\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":7,"id":"4552882b-9cc7-4877-baad-bcd140c6fc21","metadata":{"executionInfo":{"elapsed":0,"status":"aborted","timestamp":1705659875394,"user":{"displayName":"Nicola & Tom","userId":"02954767357562964474"},"user_tz":0},"id":"4552882b-9cc7-4877-baad-bcd140c6fc21"},"outputs":[],"source":["# First we load the data\n","dataset = np.load(\n","    \"/Users/nathanewer/nwe/2_dlh/dlh_practical_jan26/healthcare_practicals/data/pneumoniamnist.npz\"\n",")\n","X_train, y_train = dataset[\"train_images\"], dataset[\"train_labels\"]\n","X_val, y_val = dataset[\"val_images\"], dataset[\"val_labels\"]\n","X_test, y_test = dataset[\"test_images\"], dataset[\"test_labels\"]\n","\n","# Add your code below:"]},{"cell_type":"code","execution_count":null,"id":"0a74d751","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"4f8f7098-bd06-49fd-9491-9fc7c9cb0f63","metadata":{"id":"4f8f7098-bd06-49fd-9491-9fc7c9cb0f63"},"source":["🏥 Without medical training, it is difficult to tell whether the label given to an image is correct. Thus, we often have to trust the labels we are given but must still eyeball the data and check that there is nothing obviously strange going on and that the images contain what we expect to see.\n","\n","🏥 Considering the number of values for each label, we can see that there are many more examples with pneumonia than there are heathly subjects. This was unsurprising as the chest X-Ray was completed because of suspected pneumonia and it would be unethical to radiate someone without cause. What effect might this have on the generalisability of the model?\n","\n","For logistic regression, this data now needs to be converted into vectorised form:\n","\n","$\\mathbf{X}$ has shape: $(m_{features} \\times n_{examples})$\n","\n","We also add a row of zeros to $\\mathbf{X}$  to multiply by the bias term, $\\mathbf{w}_0$. In your machine learning course, you may have created feature vectors by generating feature maps such as edges. Here we are using the intensity values as the input features, as is the case when using CNNs.\n","\n","$\\mathbf{y}$ has shape: $1 \\times n_{examples}$"]},{"cell_type":"code","execution_count":null,"id":"12ef3123-bf8b-4036-886e-dd715211a255","metadata":{"id":"12ef3123-bf8b-4036-886e-dd715211a255"},"outputs":[],"source":["# This cell can only be run once, rerun the previous cell if you want to rerun the cell\n","X_train = X_train.reshape(X_train.shape[0], -1).T\n","X_train = np.concatenate((np.ones((1,X_train.shape[1])), X_train))   # Add a row of zeros for the bias term\n","y_train = y_train.reshape(1, y_train.shape[0])\n","print(X_train.shape, y_train.shape)\n","\n","X_val = X_val.reshape(X_val.shape[0], -1).T\n","X_val = np.concatenate((np.ones((1,X_val.shape[1])),X_val))\n","y_val = y_val.reshape(1, y_val.shape[0])\n","print(X_val.shape, y_val.shape)\n","\n","X_test = X_test.reshape(X_test.shape[0], -1).T\n","X_test = np.concatenate((np.ones((1,X_test.shape[1])),X_test))\n","y_test = y_test.reshape(1, y_test.shape[0])\n","print(X_test.shape, y_test.shape)"]},{"cell_type":"markdown","id":"8dcaca78-2c4f-4860-8af0-34a1c838a87b","metadata":{"id":"8dcaca78-2c4f-4860-8af0-34a1c838a87b"},"source":["We also want to preprocess the data to make it suitable for training the network. The best way to do this will depend on the data and the network being used, but common practice is to centre the X data such that the intensity values have zero mean and unit standard deviation.\n","\n","It is vital that the same preprocessing pipeline is applied to all subsets of the data."]},{"cell_type":"code","execution_count":null,"id":"fdc03451-3a60-4c21-bd69-382152be9602","metadata":{"id":"fdc03451-3a60-4c21-bd69-382152be9602"},"outputs":[],"source":["def centring(X):\n","    print(X.shape)\n","    epsilon = 1e-7 # To prevent division by 0\n","\n","\n","    return X\n","\n","plt.subplot(1,2,1)\n","plt.hist(X_train[:,0])\n","plt.title('Before Normalisation')\n","plt.xlabel('Intensity Value')\n","X_train = centring(X_train)\n","plt.subplot(1,2,2)\n","plt.hist(X_train[:,0])\n","plt.title('After Normalisation')\n","plt.xlabel('Intensity Value')\n","\n","X_val = centring(X_val)\n","X_test = centring(X_test)\n"]},{"cell_type":"markdown","id":"abafa2c4-b312-47ca-a238-77b6b6fbe4ac","metadata":{"id":"abafa2c4-b312-47ca-a238-77b6b6fbe4ac"},"source":["Note the changed range of intensity values after normalisation."]},{"cell_type":"markdown","id":"d8c595df-3af4-49ed-b361-f03522525f24","metadata":{"id":"d8c595df-3af4-49ed-b361-f03522525f24","tags":[]},"source":["# 2. A Single Neuron Logistic Regression Classifier\n","To understand how neural networks work, it serves to consider a single neuron as a logistic regression classifier:\n","\n","![picture](https://drive.google.com/uc?id=14gUHAZriXSEIupy99k_n8bs09Je_rtwc)\n","\n","Logistic regression can be considered as a very small neural network. Here the line $z =0$ defines a separating hyperplane, where the bias term $w_0$ has shifted this from the origin; all data points with $z >0$ are assigned to the positive class, and all data points with $z < 0$ are assigned to the negative class.\n","\n","The vector $\\mathbf{W}$ runs perpendicular to the line $z =0$ and defines the direction in which data classes are maximally separated when projected onto it.\n","\n","Our predictions for a single input can be written:\n","$$ f= f(z) = \\dfrac{1}{1+e^{-z}} $$\n","\n","Where, for logistic regression, $f$ is the sigmoid function and:\n","\n","$$z=w_0 + w_1x_1 + w_2x_2 +w_3x_3....+w_m x_m$$\n","\n","Here $w_0$ is the bias term, $w_1,w_2....w_m$ are the weights, $m$ is the number of features, and $\\mathbf{x}$ is a single example (i.e. one column) from our training set $X \\in \\mathbb{R}^{m\\times n}$ .\n","\n","✅ What are m and n in our case? Make sure you are happy with this.\n","\n","**Implementation of the forward pass:**\n","\n","We could calculate $f$ in one line of code, but it will come in handy when considering backpropagation later to consider the computation in stages, with each stage consisting of a simple module:\n","\n","$$\n","\\begin{align}\n","\\mathbf{Z} &= \\mathbf{W} \\mathbf{X} \\\\\n","\\mathbf{F}=f(\\mathbf{Z}) &= \\dfrac{1}{1+e^{-\\mathbf{Z}}}\n","\\end{align}\n","$$\n","\n"]},{"cell_type":"markdown","id":"aab05560-402f-49cd-89a5-a1c1ba6114fe","metadata":{"id":"aab05560-402f-49cd-89a5-a1c1ba6114fe"},"source":["## 2.1 Initialise $\\mathbf{W}$\n","We can now begin to construct our logistic regression. The first step is to initialise the weight matrix.\n","\n","**To do:**\n","\n","✅ Create a matrix of zeros to initialise $\\mathbf{W}$ (note initialisation by zero is ok for a single neuron).\n","\n","Hint: If $\\mathbf{X}$ has shape $(m_{features} \\times n_{examples})$, and we know that $\\mathbf{Z}$ (and thus $\\mathbf{F}$) should return _one_ scalar prediction _per example_, what shape should $\\mathbf{W}$ be?"]},{"cell_type":"code","execution_count":null,"id":"87be4a99-3843-4664-b032-ef68638a9b81","metadata":{"id":"87be4a99-3843-4664-b032-ef68638a9b81"},"outputs":[],"source":["# Add your code here:\n"]},{"cell_type":"markdown","id":"19854114-023f-4d73-b193-9e6f893e084f","metadata":{"id":"19854114-023f-4d73-b193-9e6f893e084f"},"source":["## 2.2 Estimate $\\mathbf{Z}$:\n","\n","**To do**:\n","\n","✅ Write a function $z(w,x)$ to linearly transform data matrix $\\mathbf{X}$ using the weights matrix $\\mathbf{W}$.\n","\n","**Hint** implement $\\mathbf{Z} = \\mathbf{W} \\mathbf{X}$; print out the shape - is it what you would expect?"]},{"cell_type":"code","execution_count":null,"id":"d8272688-d4ac-46b4-960d-ea20de2feb3c","metadata":{"id":"d8272688-d4ac-46b4-960d-ea20de2feb3c"},"outputs":[],"source":["def z(w,x):\n","    return\n","\n","output = z(W,X_train)\n","print(output.shape)"]},{"cell_type":"markdown","id":"276a5e35-9969-493f-8ebc-5559e962604d","metadata":{"id":"276a5e35-9969-493f-8ebc-5559e962604d"},"source":["## 2.3 Implement Sigmoid function f:\n","\n","**To do**:\n","\n","✅  Now write a function to compute $f(\\mathbf{Z})=\\dfrac{1}{1+e^{-\\mathbf{Z}}} $, our logistic regression function.\n","\n","✅  Plot the function for a range of values (-5,5) to make sure it acts as expected.\n"]},{"cell_type":"code","execution_count":null,"id":"6f159b64-036d-47e8-bb44-1e93a64a08b4","metadata":{"id":"6f159b64-036d-47e8-bb44-1e93a64a08b4"},"outputs":[],"source":["def sigmoid(z):\n","    return s\n"]},{"cell_type":"markdown","id":"6d55f5d0-f087-4a03-8e84-4772184ac516","metadata":{"id":"6d55f5d0-f087-4a03-8e84-4772184ac516"},"source":["We can now compute some predictions $\\mathbf{\\hat{y}}$, and assess the performance of the model. We aren't expecting this to be very good as the weights are all zero!"]},{"cell_type":"code","execution_count":null,"id":"4cf81a0b-5628-4398-a1a2-4670363a19c1","metadata":{"id":"4cf81a0b-5628-4398-a1a2-4670363a19c1"},"outputs":[],"source":["y_pred = sigmoid(z(W,X_train))\n","print(y_pred.shape)\n","print(y_train.shape)"]},{"cell_type":"code","execution_count":null,"id":"6adacd54-ab6c-4017-8e0a-00834af91b65","metadata":{"id":"6adacd54-ab6c-4017-8e0a-00834af91b65"},"outputs":[],"source":["def accuracy(y, y_pred, threshold = 0.5):\n","    y_pred_thresholded = y_pred > threshold\n","    correct_predictions = np.sum(y==y_pred_thresholded)\n","    total_predictions = y.shape[1]\n","    accuracy = 100 * correct_predictions / total_predictions\n","    return accuracy\n","\n","print(accuracy(y_train, y_pred))"]},{"cell_type":"markdown","id":"e54d5a5d-421f-4195-bebb-dcac55ae1991","metadata":{"id":"e54d5a5d-421f-4195-bebb-dcac55ae1991"},"source":["**To do:**\n","\n","✅ Look at the predictions ```y_pred```, what does this initial prediction return and why?"]},{"cell_type":"markdown","id":"7ea9aa3d-4d77-433f-8951-278529b6e91e","metadata":{"id":"7ea9aa3d-4d77-433f-8951-278529b6e91e"},"source":["## 2.4 Implement Cross Entropy Loss\n","We now need to optimise the weights $w$ to give good performance.\n","\n","Accuracy is easy to intepret, but can't be optimised using gradient descent. We need a measure of our prediction quality that can be optimised using gradient descent. A typical loss function used in  classification problems is cross-entropy:\n","\n","$$L(y_i,f(z_i)) = - y_i \\ln(f(z_i)) - (1-y_i) \\ln(1-f(z_i))$$\n","\n","This may be implemented using vectors as:\n","\n","$$L(\\mathbf{Y},\\mathbf{F}) = - \\mathbf{Y} \\ln(\\mathbf{F} + \\epsilon) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F} + \\epsilon)$$\n","\n","This returns a vector of losses $(L_1,L_2....L_n)$ estimated for all training examples n. The $\\epsilon$ is added for numerical stability. We require the total cost estimated, as:\n","\n","$$ J(\\mathbf{W})= \\frac{1}{n} \\sum_i L_i(y_i,f(z_i)) $$\n","\n","**To do**:\n","\n","✅ Implement the Cross-Entropy loss and return the total cost, following the equation above.\n"]},{"cell_type":"code","execution_count":null,"id":"cb6f6e31-face-4787-946c-1e03d3894981","metadata":{"id":"cb6f6e31-face-4787-946c-1e03d3894981"},"outputs":[],"source":["# Implement loss function to calculate cross-entropy loss for all examples and average to return total cost\n","# note the negative sign so that the loss decreases as our predictions get better\n","def loss(y, y_pred):\n","    eps = 1e-8\n","\n","    return J"]},{"cell_type":"code","execution_count":null,"id":"35f3fb92-57f6-46a9-badf-730aa6467152","metadata":{"id":"35f3fb92-57f6-46a9-badf-730aa6467152"},"outputs":[],"source":["total_loss = loss(y_train, y_pred)\n","print(total_loss)"]},{"cell_type":"markdown","id":"5d3283a2-958b-421e-bf64-bf3a8249ecb8","metadata":{"id":"5d3283a2-958b-421e-bf64-bf3a8249ecb8"},"source":["## 2.5 Computation Graph\n","\n","Now we have our functions for $\\mathbf{L}$ and $\\mathbf{Z}$, and initialised $\\mathbf{W}$, we are finally in a position to compute a forward and backward pass. Computation graphs can help us to achieve this by tracking the order of operations. The computation graph for logistic regression is:\n","\n","<img src=\"https://drive.google.com/uc?id=1R8rx5PerBAgMOJlCx-ordI6jGAcH-uCP\" width=\"1000\" height=\"400\">\n","\n","Don't forget that the full cost equates to the mean of the loss over all examples $J=\\frac{1}{n_T}\\sum_i L_I$ , $\\dfrac{dJ}{dW}=\\frac{1}{n_T} \\sum_i \\dfrac{dL_i}{dW} $ ."]},{"cell_type":"markdown","id":"0ba1a31f-d99f-4245-906b-45ce00748bd7","metadata":{"id":"0ba1a31f-d99f-4245-906b-45ce00748bd7"},"source":["## 2.6 Implement the forward pass\n","We now have all the components of the forward pass for our logistic regression.\n","\n","**To do**:\n","\n","✅ Write a full forward pass that takes data, targets and a weight matrix and performs the forward pass, using vectors."]},{"cell_type":"code","execution_count":null,"id":"b8fa5aa6-fa50-413c-8614-a2da664ebcce","metadata":{"id":"b8fa5aa6-fa50-413c-8614-a2da664ebcce"},"outputs":[],"source":["def forward_pass(X, y, W):\n","\n","\n","    print('Loss: {}'.format(loss(y, y_pred)))\n","    print('Accuracy: {}'.format(accuracy(y ,y_pred)))\n","    return y_pred, l\n","\n","y_pred, l = forward_pass(X_train,y_train, W)\n"]},{"cell_type":"markdown","id":"2d7b7677-4736-4d0b-adcc-494eec5bc754","metadata":{"id":"2d7b7677-4736-4d0b-adcc-494eec5bc754"},"source":["## 2.7 Implement the backwards pass\n","We're now ready to try to adjust our parameters $\\mathbf{W}$ in order to optimise our predictions. To do this, we need to calculate the change in our loss function with respect to our parameters, $\\dfrac{\\partial L}{\\partial \\mathbf{W}}$.\n","\n","Recalling our staged calculation of the logistic regression (in vectorised form):\n","\n","$$\n","\\mathbf{Z} = \\mathbf{W} \\mathbf{X} \\\\\n","\\mathbf{F}= \\dfrac{1}{1+e^{- \\mathbf{Z}}} \\\\\n","\\mathbf{L}  =  - \\mathbf{Y} \\ln(\\mathbf{F}) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F})\n","$$\n","\n","We can write the vectorised gradients for each individual stage:\n","\n","$$\n","\\dfrac{\\partial L}{\\partial f} = \\dfrac{\\mathbf{F} - \\mathbf{Y}}{\\mathbf{F}(1-\\mathbf{F})}\\\\\n","\\dfrac{\\partial f}{\\partial z} = \\mathbf{F}(1-\\mathbf{F}) \\\\\n","\\dfrac{\\partial z}{\\partial w} = \\mathbf{X}^T\n","$$\n","\n","And compose through the chain rule:\n","\n","$$\n","\\dfrac{\\partial L}{\\partial w} = \\dfrac{\\partial L}{\\partial f} \\cdot \\dfrac{\\partial f}{\\partial z} \\cdot\\dfrac{\\partial z}{\\partial w} \\\\\n","\\dfrac{\\partial L}{\\partial w} = \\dfrac{\\mathbf{F} - \\mathbf{Y}}{\\mathbf{F}(1-\\mathbf{F})} \\cdot \\mathbf{F}(1-\\mathbf{F}) \\cdot \\mathbf{X}^T\n","$$\n","\n","Which can be simplified by cancelling $ \\mathbf{F}(1-\\mathbf{F})$ terms in both the numerator and the denominator:\n","\n","$$ \\dfrac{\\partial L}{\\partial w} = (\\mathbf{F} - \\mathbf{Y}) \\mathbf{X}^T $$\n","\n","Let's calculate the gradient of our loss, $\\dfrac{\\partial L}{\\partial \\mathbf{W}}$, for a **single** input, $\\mathbf{x}$.\n","\n","**To do:**\n","\n","✅ Work through the equations above and be sure you are comfortable with all of the steps. Refer to 2.5 and the computation graph.\n","\n","✅ Fill in the calculations of the backwards pass."]},{"cell_type":"code","execution_count":null,"id":"6f512643-ff1c-4204-863f-f2a899250b48","metadata":{"id":"6f512643-ff1c-4204-863f-f2a899250b48"},"outputs":[],"source":["W = np.zeros((1,X_train.shape[0]))\n","\n","# select just the first example here\n","x_single = X_train[:,0:1]\n","y_single = y_train[:,0:1]\n","print('The true value of y is: {}'.format(y_single))\n","\n","# calculate the forward pass, and store the outputs at each stage\n","Z = z(W,x_single)\n","F = sigmoid(Z)\n","print('Our prediction for y is: {}'.format(F))\n","\n","l = loss(y_single,F)\n","print('The loss is {}'.format(l))\n","\n","# now enter the backwards pass here, implementing using the equations above:\n","dl_dw =\n","\n","print(dl_dw.shape,W.shape,x_single.shape)"]},{"cell_type":"markdown","id":"5ae04347-4bc2-4577-9bf9-de448d59d11e","metadata":{"id":"5ae04347-4bc2-4577-9bf9-de448d59d11e"},"source":["We can check if this gradient calculation is correct by updating our weights vector and looking at our new prediction:"]},{"cell_type":"code","execution_count":null,"id":"126d060b-8d62-45c5-8e9d-5bcec731e9b0","metadata":{"id":"126d060b-8d62-45c5-8e9d-5bcec731e9b0"},"outputs":[],"source":["# The gradient is in the direction of increasing loss, so we subtract the gradient from w.\n","W = W - 0.001 * dl_dw\n","Z = z(W,x_single)\n","F = sigmoid(Z)\n","l = loss(y_single,F)\n","print('Our updated prediction for y is: {}'.format(F))\n","print('The loss is {}'.format(l))"]},{"cell_type":"markdown","id":"c974afc1-7f40-4700-97cd-4e55a4723573","metadata":{"id":"c974afc1-7f40-4700-97cd-4e55a4723573"},"source":["Has the loss decreased? If not, check your answer. We have only updated $\\mathbf{w}$ using information from a single data point. In practice, we want to use all of the data available for training. The following implements the gradient calculation for all data points. Make sure you understand what is happening here:"]},{"cell_type":"code","execution_count":null,"id":"39bbab78-a54a-4da0-82ed-77390c86c52e","metadata":{"id":"39bbab78-a54a-4da0-82ed-77390c86c52e"},"outputs":[],"source":["w = np.zeros((1,X_train.shape[0])) # We are now using all of the training data\n","\n","# calculate the forward pass, and store the outputs at each stage\n","Z = z(w,X_train)\n","F = sigmoid(Z)\n","l = loss(y_train, F)\n","\n","# To do - implement the backward pass\n","dl_dw = np.matmul((F-y_train),X_train.T)\n","\n","print('dl_dw has shape: {}'.format(dl_dw.shape))\n","\n","n_examples=X_train.shape[1]\n","grad_mean = dl_dw/n_examples"]},{"cell_type":"markdown","id":"72afe3d2-a543-4a6e-afbc-9f39b0880116","metadata":{"id":"72afe3d2-a543-4a6e-afbc-9f39b0880116"},"source":["## 2.8 The training loop\n","\n","We now have everything we need to train a logistic regression classifier using backprop.\n","\n","**To do:**\n","\n","✅ Create a training loop using the code from this practical so far.\n","\n","✅ Plot the training curves to assess the training progression.\n","\n","✅ Try changing the learning rate to see what effect this has on the training."]},{"cell_type":"code","execution_count":null,"id":"9effd4a6-5d89-4f8d-8583-119cf4d687b9","metadata":{"id":"9effd4a6-5d89-4f8d-8583-119cf4d687b9"},"outputs":[],"source":["# initialise w to all zeros\n","w = np.zeros((1,X_train.shape[0]))\n","epsilon=1e-5\n","\n","# we'll store the loss and accuracy in these lists during training\n","loss_record = []\n","accuracy_record = []\n","\n","num_iterations = 100\n","learning_rate = 1e-2\n","\n","for i in range(num_iterations):\n","    # forward pass - get predictions\n","    Z =\n","    F =\n","    l = loss(y_train, F)\n","\n","    # store the loss/ accuracy at this iteration\n","    accuracy_it=accuracy(y_train,F)\n","    loss_record.append(l)\n","    accuracy_record.append(accuracy_it)\n","\n","    #backwards pass to get gradients\n","    dL_dw =\n","    grad_mean = dL_dw/X_train.shape[1]\n","\n","    # update the weights\n","    w ="]},{"cell_type":"code","execution_count":null,"id":"0559ce7d-2e99-4557-83cf-81df6ecd2fb5","metadata":{"id":"0559ce7d-2e99-4557-83cf-81df6ecd2fb5"},"outputs":[],"source":["# Plot the training graphs using matplotlib\n"]},{"cell_type":"markdown","id":"833e594e-8338-4c27-befb-4f1762b59a9b","metadata":{"id":"833e594e-8338-4c27-befb-4f1762b59a9b"},"source":["## 2.8 Testing on held out test set.\n","\n","We now want to test our trained model on the test data. This data has not been seen by the model and it's important to report the performance on unseen data, rather than the data used to train the model."]},{"cell_type":"code","execution_count":null,"id":"aa268fb8-4bc4-4987-af5e-df7f836bbcf5","metadata":{"id":"aa268fb8-4bc4-4987-af5e-df7f836bbcf5"},"outputs":[],"source":["Z_test = z(w,X_test)\n","F_test = sigmoid(Z_test)\n","l = loss(y_test,F_test)\n","\n","print(l,accuracy(y_test,F_test))"]},{"cell_type":"markdown","id":"a9521a7c-0ab7-4697-a982-7ddc4610e69e","metadata":{"id":"a9521a7c-0ab7-4697-a982-7ddc4610e69e"},"source":["Not bad for such a simple model! Next week we will try ANNs/CNNs on the same data and see if we can improve the performance.\n","\n","(Note the slight drop in performance from the training data. This is to be expected as this is unseen data, but if the drop is much larger it would suggest that the model has overfit to the training data. We will explore this more in future practicals.)"]},{"cell_type":"markdown","id":"fdce5848-ad16-419c-a7dd-0aec6b1d5696","metadata":{"id":"fdce5848-ad16-419c-a7dd-0aec6b1d5696"},"source":["🏥 We have so far only evaluated the accuracy, but other metrics may be more important for medical applications. For instance, we may be most concerned about minimising the false negative (FN) rate.\n","\n","**To do:**\n","\n","✅ Implement a function to measure FN rate (remember Label 0 = healthy, Label 1 = Pneumonia)"]},{"cell_type":"code","execution_count":null,"id":"e76a0a42-7ff9-443a-ab13-bc1bc1417e4e","metadata":{"id":"e76a0a42-7ff9-443a-ab13-bc1bc1417e4e"},"outputs":[],"source":["def FN_rate(y, y_pred, threshold = 0.5):\n","\n","    return FN_rate\n","\n","print(FN_rate(y_test, F_test))"]},{"cell_type":"markdown","id":"3c982c2f","metadata":{"id":"3c982c2f"},"source":["# 3. Extra Considerations\n","\n","🏥 What is the differences between linear and logistic regression? Which would you expect to be most appropriate for the images we considered in this practical?\n","\n","🏥 What would you expect to happen to model training if:\n","- The dataset became more imbalanced?\n","- The image size increased?\n","\n","🏥 In this practical we used a threshold of 0.5 to binarise the prediction result. How else could we have selected the threshold?\n","\n","🏥 In these examples we were able to optimise learning the whole dataset. If $N$, the number of training datapoints, is very large, this may not be possible. How does stochastic gradient descent allow us to train models?\n","\n","🏥 Medical images are often very large and 3D. What effect is this likely to have on model training.\n","\n","🏥 Explain when minimising the number of false negatives be critical?\n","\n","🏥 What stategies might you employ to cope with the class imbalance in the dataset?\n","\n","🏥 What is the difference between the sigmoid and softmax functions?\n","\n"]},{"cell_type":"markdown","id":"45ed9b24","metadata":{"id":"45ed9b24"},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":5}
